package it.cnr.isti.pad;

import java.io.IOException;
import java.io.OutputStream;
import org.apache.commons.io.LineIterator;
import java.util.*;
import org.apache.hadoop.fs.FileSystem;
import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
import it.cnr.isti.pad.Parser.*;

public class Pagerank
{
	//------------------------------------------Calc PR-----------------------------------------------
	public static class Dangling extends Mapper<Text, PairWritable, Text, PairWritable>
	{
		private int scale_factor;
		static enum Counter{
			SINK
 		}

		public void map(Text key, PairWritable value, Context context) throws IOException, InterruptedException
		{
			if(value.getAdj_list().toString().isEmpty())
			{
				context.getCounter(Counter.SINK).increment((int)(value.getPagerank().get()*scale_factor));
			}
		}
		@Override
		protected void setup(Context context) throws IOException, InterruptedException {
			this.scale_factor = context.getConfiguration().getInt("SCALE", 10000);
		}
	}

	public static class Map extends Mapper<Text, PairWritable, Text, PairWritable>
	{
		public void map(Text key, PairWritable value, Context context) throws IOException, InterruptedException
		{
			PairWritable map_value = new PairWritable();
			Text map_key = new Text();
			context.write(key,value);
			System.out.println(key+"	"+value.getPagerank().get()+"	"+value.getAdj_list().toString()+"	"+value.isNode().get());
			if(!value.getAdj_list().toString().isEmpty())
			{
				///map_value.set(value.getPagerank().get(),value.getAdj_list().toString());
				String[] adj_list = value.getAdj_list().toString().split("[ ]");
				for (String s: adj_list)
				{
					map_key.set(s);
					map_value.set(value.getPagerank().get()/adj_list.length,"");
					map_value.setNode(false);
					context.write(map_key,map_value);
					System.out.println(map_key.toString()+"	"+map_value.getPagerank().get()+"	"+map_value.getAdj_list().toString()+"	"+map_value.isNode().get());
				}
			}
		}

	}
	public static class Red extends Reducer<Text, PairWritable, Text, PairWritable>
	{
		private int nodes_number;
		private double lambda;
		private int scale_factor;
		private int sink;
		private double dangling;
		public static enum Counter {
			CONV,
			SUM
		}

		public void reduce(Text key, Iterable<PairWritable> values, Context context) throws IOException, InterruptedException
		{
			Text red_key = new Text();
			PairWritable red_value = new PairWritable();
			double sum = 0;
			double pagerank = 0;
			String adj_list="";
			for (PairWritable val:values){
				if(!val.isNode().get()){
					sum+=val.getPagerank().get();
				}else{
					adj_list=val.getAdj_list().toString();
					pagerank=val.getPagerank().get();
				}
			}
			sum = ((1-lambda)/nodes_number)+((lambda)*((dangling/nodes_number)+((1-lambda)/nodes_number)+((lambda)*(sum))));
			context.getCounter(Counter.CONV).increment((int)(Math.abs(sum-pagerank)*scale_factor));
			context.getCounter(Counter.SUM).increment((int)(sum*scale_factor));
			red_value.set(sum,adj_list);
			red_value.setNode(true);
			context.write(key ,red_value);
			//System.out.println(key.toString()+"	"+red_value.getPagerank().get()+"	"+red_value.getAdj_list().toString()+"	"+red_value.isNode().get());
		}
		@Override
		protected void setup(Context context) throws IOException, InterruptedException {
			this.nodes_number = context.getConfiguration().getInt("NODES_NUMBER", 0);
			this.lambda = Double.parseDouble(context.getConfiguration().get("LAMBDA", "0.8"));
			this.scale_factor = context.getConfiguration().getInt("SCALE", 10000);
			this.dangling = Double.parseDouble(context.getConfiguration().get("DANGLING", "0"));
		}
	}
//-------------------------------------------Sort------------------------------------------------
	public static class Sorter extends Mapper<Text, PairWritable, DoubleWritable, Text>
	{
		public void map(Text key, PairWritable value, Context context) throws IOException, InterruptedException
		{  	
			context.write(value.getPagerank(),key);
		}
	}
//----------------------------------------------------------------------------------------------------------------------
public static int getNumNodes(Path input_file) throws IOException {
		Configuration conf = new Configuration();
		FileSystem fs = input_file.getFileSystem(conf);
		LineIterator lineIterator = IOUtils.lineIterator(fs.open(input_file), "UTF8");
		int lines=0;
		while ( lineIterator.hasNext() ) {
			lines++;
			lineIterator.nextLine();
		}
		return lines;
	}

	public static void main(String[] args) throws Exception
	{	
		double req_convergence = 0.01;
		double convergence = 0;
		double lambda = 0.85;
		int nodes_number=0;
		Path pagelink_inputPath=null;
		Path outputPath=null;
		int max_iter=10;

		Configuration conf = new Configuration();
		if(args.length>2){
			Path page_inputPath = new Path(args[0]);
			pagelink_inputPath= new Path(args[1]);
			outputPath = new Path(args[2]);
			nodes_number = getNumNodes(pagelink_inputPath);

			outputPath.getFileSystem(conf).delete(outputPath, true);

			Job job_parser = new Job(conf, "Parse Page");

			Path parsedPageOutputPath = new Path(outputPath,"parsed_"+args[0].split("\\.")[0]);
			job_parser.setJarByClass(Parser.class);
			job_parser.setOutputKeyClass(Text.class);
			job_parser.setOutputValueClass(Text.class);
			job_parser.setMapperClass(PageParser.class);
			job_parser.setInputFormatClass(KeyValueTextInputFormat.class);
			FileInputFormat.setInputPaths(job_parser, page_inputPath);
			FileOutputFormat.setOutputPath(job_parser, parsedPageOutputPath);
			job_parser.waitForCompletion(true);

			Path parsedPagelinkOutputPath = new Path(outputPath,"parsed_"+args[1].split("\\.")[0]);
			job_parser = new Job(conf, "Parse Pagelink");
			job_parser.getConfiguration().setInt("NODES_NUMBER", nodes_number);
			job_parser.setJarByClass(Parser.class);
			job_parser.setOutputKeyClass(Text.class);
			job_parser.setOutputValueClass(PairWritable.class);
			job_parser.setMapperClass(PagelinkParser_Map.class);
			job_parser.setReducerClass(PagelinkParser_Red.class);
			MultipleInputs.addInputPath(job_parser, parsedPageOutputPath, KeyValueTextInputFormat.class, PagelinkParser_Map.class);
			MultipleInputs.addInputPath(job_parser, pagelink_inputPath, KeyValueTextInputFormat.class, PagelinkParser_Map.class);
			job_parser.setOutputFormatClass(SequenceFileOutputFormat.class);
			FileOutputFormat.setOutputPath(job_parser, parsedPagelinkOutputPath);
			job_parser.waitForCompletion(true);
			pagelink_inputPath = parsedPagelinkOutputPath;
		}else  if(args.length==2){
			pagelink_inputPath = new Path(args[0]);
			nodes_number = getNumNodes(pagelink_inputPath);
			outputPath = new Path(args[1]);
			outputPath.getFileSystem(conf).delete(outputPath, true);
			Job job_parser = Job.getInstance(conf, "Parse Text");
			job_parser.getConfiguration().setInt("NODES_NUMBER", nodes_number);
			job_parser.setJarByClass(Parser.class);
			job_parser.setOutputKeyClass(Text.class);
			job_parser.setOutputValueClass(PairWritable.class);
			job_parser.setMapperClass(Simple_Parser.class);
			job_parser.setNumReduceTasks(0);
			job_parser.setInputFormatClass(KeyValueTextInputFormat.class);
			job_parser.setOutputFormatClass(SequenceFileOutputFormat.class);
			FileInputFormat.setInputPaths(job_parser, pagelink_inputPath);
			Path parsedOutputPath = new Path(outputPath,"parsed_"+args[1].split("\\.")[0]);
			FileOutputFormat.setOutputPath(job_parser, parsedOutputPath);
			job_parser.waitForCompletion(true);
			pagelink_inputPath=parsedOutputPath;

		}else{
			System.out.println("Error");
			System.exit(0);
		}
		int scale_factor = (int) (100*(Math.pow(10,Math.floor(Math.log10(nodes_number) + 1))));

		int iter=0;
		while (true) {
			Job job = Job.getInstance(conf, "PageRank Dangling Calc"+iter);
			job.getConfiguration().setInt("SCALE", scale_factor);
			job.setJarByClass(Pagerank.class);
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(PairWritable.class);
			job.setMapperClass(Dangling.class);
			job.setNumReduceTasks(0);
			job.setInputFormatClass(SequenceFileInputFormat.class);
			job.setOutputFormatClass(NullOutputFormat.class);
			FileInputFormat.setInputPaths(job, pagelink_inputPath);
			job.waitForCompletion(true);

			double dangling = (double)(job.getCounters().findCounter(Dangling.Counter.SINK).getValue())/(scale_factor);
			System.out.println(dangling);

			job = Job.getInstance(conf, "PageRank Calc "+iter);
			job.getConfiguration().setInt("NODES_NUMBER", nodes_number);
			job.getConfiguration().set("DANGLING", Double.toString(dangling));
			job.getConfiguration().set("LAMBDA", Double.toString(lambda));
			job.getConfiguration().setInt("SCALE", scale_factor); 
			job.setJarByClass(Pagerank.class);
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(PairWritable.class);
			job.setMapperClass(Map.class);
			job.setReducerClass(Red.class);
			job.setInputFormatClass(SequenceFileInputFormat.class);
			job.setOutputFormatClass(SequenceFileOutputFormat.class);
			FileInputFormat.setInputPaths(job, pagelink_inputPath);
			Path jobOutputPath = new Path(outputPath, String.valueOf(iter));
			FileOutputFormat.setOutputPath(job, jobOutputPath);
			job.waitForCompletion(true);
			//pagelink_inputPath.getFileSystem(conf).delete(pagelink_inputPath, true);
			convergence = ((double)(job.getCounters().findCounter(Red.Counter.CONV).getValue()))/((nodes_number*scale_factor));
			System.out.println(convergence);
			System.out.println((double)(job.getCounters().findCounter(Red.Counter.SUM).getValue())/(scale_factor));
			pagelink_inputPath = jobOutputPath;
			iter++;
			if((convergence<req_convergence)||(iter>max_iter)){
				break;
			}
			
	    }
		Job job_sorter = new Job(conf, "Sorter_job");
		job_sorter.setJarByClass(Pagerank.class);
		job_sorter.setOutputKeyClass(DoubleWritable.class);
		job_sorter.setOutputValueClass(Text.class);
		job_sorter.setMapperClass(Sorter.class);
		job_sorter.setInputFormatClass(SequenceFileInputFormat.class);
		job_sorter.setSortComparatorClass(DescendingKeyComparator.class);
		FileInputFormat.setInputPaths(job_sorter, pagelink_inputPath);
		FileOutputFormat.setOutputPath(job_sorter, new Path(outputPath,"sorted_"+pagelink_inputPath));
		job_sorter.waitForCompletion(true);
		//inputPath.getFileSystem(conf).delete(pagelink_inputPath, true);
	}
}